from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.microsoft.mssql.operators.mssql import MsSqlOperator
from airflow.exceptions import AirflowSkipException
from datetime import datetime, timedelta
import pandas as pd
import requests
from io import StringIO
from sqlalchemy import create_engine, text,MetaData,Table,Column, Integer, String, DECIMAL

default_args = {
    'owner':'Ahmed_marzouk',
    'depends_on_past': False,
    'start_date': datetime(2020,1,20),
    'end_date': datetime(2023, 3, 30),  # Add this line
    'retries':3,
    'retry_delay': timedelta(minutes=5)
}

MSSQL_CONN_STR = (
    "mssql+pyodbc://airflow_user:5658545212@"
    ".\\SQLEXPRESS/Covid2?"
    "driver=ODBC+Driver+18+for+SQL+Server&"
    "TrustServerCertificate=yes&"
    "Encrypt=no&"
    "autocommit=True&"
    "fast_executemany=True"
)



JHU_BASE_URL = "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/"
COUNTRY_MAPPING = {
    'US': 'United States',
    'USA': 'United States',
    'Korea, South': 'South Korea',
    'UK': 'United Kingdom',
    'Russian Federation' : 'Russia',
    'Mainland China' : 'China',
    'Republic Of Korea' : 'South Korea',
    'Iran (Islamic Republic Of)' : 'Iran',
    'Bahamas, The' : 'Bahamas',
    'Korea, North' : 'North Korea',
    'Czechia' : 'Czech Republic',
    'Dominican Republic' : 'Dominica',
    'Gambia, The' : 'Gambia',
    'Hong Kong Sar' : 'Hong Kong',
    'Macao Sar' : 'Macau',
    'Occupied Palestinian Territory' : 'Palestine',
    'Republic Of Ireland' : 'Ireland',
    'Taiwan*' :'Taiwan',
    'The Bahamas' : 'Bahamas',
    'The Gambia' : 'Gambia',
    'Viet Nam' : 'Vietnam',
    'West Bank And Gaza' : 'Palestine'

}

SCHEMA_SQL = """
USE Covid2;  -- Ensure we're in the correct database


IF NOT EXISTS (SELECT 1 FROM sys.tables WHERE name = 'dim_date' AND schema_id = SCHEMA_ID('dbo'))
BEGIN
    CREATE TABLE dbo.dim_date (
        date_id INT IDENTITY(1,1) PRIMARY KEY,
        full_date DATE NOT NULL UNIQUE,
        day INT NOT NULL,
        month INT NOT NULL,
        year INT NOT NULL,
        week_of_year INT NOT NULL
    );
END

IF NOT EXISTS (SELECT 1 FROM sys.tables WHERE name = 'dim_location' AND schema_id = SCHEMA_ID('dbo'))
BEGIN
    CREATE TABLE dbo.dim_location (
        location_id INT IDENTITY(1,1) PRIMARY KEY,
        country VARCHAR(255) NOT NULL,
        CONSTRAINT UQ_location UNIQUE (country)
    );
END

IF NOT EXISTS (SELECT 1 FROM sys.tables WHERE name = 'fact_covid_cases' AND schema_id = SCHEMA_ID('dbo'))
BEGIN
    CREATE TABLE dbo.fact_covid_cases (
        case_id INT IDENTITY(1,1) PRIMARY KEY,
        date_id INT NOT NULL,
        location_id INT NOT NULL,
        confirmed INT CHECK (confirmed >= 0),
        deaths INT CHECK (deaths >= 0),
        recovered INT CHECK (recovered >= 0),
        FOREIGN KEY (date_id) REFERENCES dbo.dim_date(date_id),
        FOREIGN KEY (location_id) REFERENCES dbo.dim_location(location_id)
    );
END
"""

def standarize_column_names(df):

        column_map = {
        'Country/Region': 'country',
        'Country_Region': 'country',
        'Last Update': 'Last_Update',
        'Last_Update': 'Last_Update',
        'Confirmed': 'Confirmed',
        'Deaths': 'Deaths',
        'Recovered': 'Recovered'
    }

        df = df.rename(columns = {k :v for k,v in column_map.items() if k in df.columns})


        return df

         



def fetch_data(**context):
        execution_date = context['execution_date']
        date_str = execution_date.strftime("%m-%d-%Y")

        try:
                jhu_url = f"{JHU_BASE_URL}{date_str}.csv"
                response = requests.get(jhu_url,timeout = 30)

                if response.status_code == 404:
                        raise AirflowSkipException(f"No Jhu data for {date_str}")
                response.raise_for_status()

                jhu_df = pd.read_csv(StringIO(response.text))

                return jhu_df.to_json()
                
               
        except Exception as e:
                context['ti'].log.error(f"Failed to fetch data: {str(e)}")
                raise

def clean_transform_data(**context):
        ti = context['ti']
        raw_data = ti.xcom_pull(task_ids = 'fetch_data')

        if not raw_data:
                raise AirflowSkipException("no data")

        df = pd.read_json(raw_data)
        df = standarize_column_names(df)

     

        

        df['date']=pd.to_datetime(df['Last_Update'],errors='coerce')
        df=df.dropna(subset=['date'])
        
        df['country'] = df['country'].replace(COUNTRY_MAPPING)

        df['country'] = df['country'].str.strip().str.title().str.replace(' Of ', ' of ')

        df['country'] = df['country'].fillna('unknown')
        df['country'] = df['country'].replace('', 'unknown')
        clean_df = df.groupby(['date','country'],as_index= False).agg({
                'Confirmed' : 'sum',
                'Deaths' : 'sum',
                'Recovered' : 'sum'
        })

        metrics = ['Confirmed', 'Deaths', 'Recovered']

        df[metrics] = df[metrics].fillna(0).clip(lower = 0)



        return clean_df.to_json(date_format='iso')

def load_dimensions(**context):
        ti = context['ti']
        engine = create_engine(MSSQL_CONN_STR,fast_executemany=True)

        clean_data = ti.xcom_pull(task_ids = 'clean_transform_data')

        if not clean_data:
                raise AirflowSkipException('no clean data')

        df = pd.read_json(clean_data)

        df['full_date'] = pd.to_datetime(df['date']).dt.date
        dates = df[['full_date']].drop_duplicates()

        dates['full_date_dt']= pd.to_datetime(dates['full_date'])
        dates['day'] = dates['full_date_dt'].dt.day
        dates['month'] = dates['full_date_dt'].dt.month
        dates['year'] = dates['full_date_dt'].dt.year
        dates['week_of_year'] = dates['full_date_dt'].dt.isocalendar().week
        
        
        date_records = dates[['full_date','day','month','year','week_of_year']]

        

        with engine.begin() as connection:

                connection.execute(text("CREATE TABLE #temp_dates(full_date DATE, day INT, month INT, year INT, week_of_year INT)"))
                ti.log.info(f"Loading {len(date_records)} dates into temp table")
                date_records.to_sql('#temp_dates',connection,if_exists= 'append',index= False,method='multi',chunksize= 1000)
                existing_check = text("""SELECT COUNT(*)
                                      FROM dim_date 
                                      WHERE full_date in (SELECT full_date FROM #temp_dates)""")
                existing_count = connection.execute(existing_check).scalar()
                ti.log.info(f"Found {existing_count} existing dates in dim_date")

                connection.execute(text("""MERGE INTO  dim_date AS target
                                        using #temp_dates AS source
                                        on target.full_date = source.full_date
                                        WHEN NOT MATCHED THEN
                                                INSERT (full_date, day, month, year, week_of_year)
                                                VALUES (source.full_date, source.day, source.month, source.year, source.week_of_year);
                                        
                                        """))     
                connection.execute(text("DROP TABLE #temp_dates"))

                #--- locaation dim
                locations = df[['country']].copy()

                locations['country'] = locations['country'].str.upper().str.strip()
                
                

                locations = locations.drop_duplicates()

                locations['country'] = locations['country'].str.title()



                

                dupes_in_pandas = locations.duplicated(subset=['country']).sum()
                if dupes_in_pandas > 0:
                        ti.log.error(f"Found {dupes_in_pandas} duplicates after Pandas deduplication")
                        sample_dupes = locations[locations.duplicated(subset=['country'], keep=False)]
                        ti.log.error(f"Sample duplicates:\n{sample_dupes.head(5)}")
                        raise ValueError("Duplicate country-province pairs exist in cleaned data")
                
                locations = locations[['country']]

                



                connection.execute(text("""
                        CREATE TABLE #temp_locations (
                                country VARCHAR(255) COLLATE SQL_Latin1_General_CP1_CI_AS
                        );
                        """))

                locations.to_sql('#temp_locations',connection,index = False,if_exists = 'append',chunksize= 1000)

                dupe_check =text("""SELECT country, count(*)
                                         FROM #temp_locations
                                         GROUP BY country 
                                         HAVING count(*) > 1
                                         """)
                dupes = connection.execute(dupe_check).fetchall()
                if dupes:
                        connection.execute(text("DROP TABLE #temp_locations"))
                        raise ValueError(f"Title-case duplicates found: {dupes}")
        
                

                connection.execute(text("""
            MERGE INTO dim_location AS target
            USING #temp_locations AS source
            ON target.country = source.country COLLATE SQL_Latin1_General_CP1_CI_AS
            WHEN NOT MATCHED THEN 
                INSERT (country) VALUES (source.country);
        """))

                connection.execute(text("DROP TABLE #temp_locations"))
        ti.log.info(f"Successfully loaded {len(date_records)} dates and {len(locations)} locations")

                

                        


def load_facts(**context):
        ti = context['ti']
        engine = create_engine(MSSQL_CONN_STR,fast_executemany=True)
        
        clean_data = ti.xcom_pull(task_ids='clean_transform_data')
        if not clean_data:
                raise AirflowSkipException('No clean data')
        
        df = pd.read_json(clean_data)
 
        
        # Convert 'date' to datetime
        df['date'] = pd.to_datetime(df['date']).dt.date
        
        # Read date map with explicit type
        try:
                date_map = pd.read_sql("SELECT date_id, full_date FROM dbo.dim_date", engine)
                date_map['full_date'] = pd.to_datetime(date_map['full_date']).dt.date
                loc_map = pd.read_sql("SELECT location_id, country FROM dbo.dim_location", engine)
        except Exception as e:
                 ti.log.error("Dimension tables missing! Run dimension pipeline first")
                 raise

        
        # Merge with proper types
        fact_df = df.merge(
        date_map, 
        left_on='date', 
        right_on='full_date',
        how = 'inner')

        ti.log.info(f"After date merge: {len(fact_df)} records")

        fact_df = fact_df.merge(loc_map,on = ['country'],how='inner')
        
        ti.log.info(f"After location merge: {len(fact_df)} records")

        fact_df = fact_df[['date_id', 'location_id', 'Confirmed', 'Deaths', 'Recovered']]
        # After merging data:
        fact_df = fact_df.rename(columns={
        'Confirmed': 'confirmed',
        'Deaths': 'deaths',
        'Recovered': 'recovered'
        })
 
        
        fact_df = fact_df.dropna(subset=['date_id'])

        fact_df['date_id'] = fact_df['date_id'].astype(int)
        fact_df['location_id'] = fact_df['location_id'].astype(int)
        for col in ['confirmed', 'deaths', 'recovered']:
                fact_df[col] = fact_df[col].fillna(0).astype(int)

        fact_df  = fact_df.drop_duplicates(subset = ['date_id','location_id'])

        ti.log.info(f"Final fact data shape: {fact_df.shape}")
        if fact_df.empty:
                raise AirflowSkipException("No data to load into fact_covid_cases")
        


        with engine.begin() as connection:

                connection.execute(text("""
              
            CREATE TABLE #temp_facts (
                date_id INT NOT NULL,
                location_id INT NOT NULL,
                confirmed INT ,
                deaths INT ,
                recovered INT 
            )
        """))
                
               
        # Insert data
       
          # Define temp table manually
                metadata = MetaData()
                temp_table = Table(
                        '#temp_facts', metadata,
                        Column('date_id', Integer, nullable=False),
                        Column('location_id', Integer, nullable=False),
                        Column('confirmed', Integer),
                        Column('deaths', Integer),
                        Column('recovered', Integer)
                )

                # Insert data in chunks
                data = fact_df.to_dict('records')
                chunk_size = 2000
                for i in range(0, len(data), chunk_size):
                        chunk = data[i:i + chunk_size]
                        connection.execute(temp_table.insert(), chunk)

                dup_check = connection.execute(text("""
                SELECT date_id, location_id, COUNT(*) 
                FROM #temp_facts 
                GROUP BY date_id, location_id 
                HAVING COUNT(*) > 1
            """)).fetchall()
                
                if dup_check:
                        ti.log.error(f"duplicates rows detected{dup_check}")
                        raise ValueError("duplicates rows in temp table")

                connection.execute(text(""" MERGE INTO fact_covid_cases  AS target
                                  USING (SELECT DISTINCT date_id,location_id,confirmed,deaths,recovered
                                        FROM #temp_facts) AS source
                                  ON source.date_id = target.date_id AND
                                  source.location_id = target.location_id
                                  WHEN MATCHED AND (
                                        target.confirmed <> source.confirmed
                                        OR target.deaths <> source.deaths
                                        OR target.recovered <> source.recovered
                                  ) THEN
                                  UPDATE SET 
                                        target.confirmed = source.confirmed,
                                        target.deaths = source.deaths,
                                        target.recovered = source.recovered
                                  WHEN NOT MATCHED THEN
                                  INSERT(date_id, location_id, confirmed, deaths, recovered)
                                  VALUES (source.date_id, source.location_id, 
                                  source.confirmed, source.deaths, source.recovered);"""))
                
                
                
                




                
                
with DAG('Covid_analytics',default_args=default_args,
    schedule_interval='0 3 * * *',

    end_date=datetime(2023, 3, 30),  # Daily at 3am
    catchup=True,

    ) as dag:
        
        create_table = MsSqlOperator(
        task_id = 'create_table',
        mssql_conn_id = 'mssql_default',
         sql=SCHEMA_SQL)
    
        fetch_dataa = PythonOperator(
        task_id = 'fetch_data',
        python_callable = fetch_data,
        op_kwargs={"context": "{{ execution_date }}"}
         )

        clean_data = PythonOperator(
                task_id = 'clean_transform_data',
                python_callable = clean_transform_data,
                op_kwargs={"context": "{{ execution_date }}"}
                )

        load_dim = PythonOperator(task_id = 'load_dimensions',
                                python_callable = load_dimensions,
                                op_kwargs={"context": "{{ execution_date }}"})

        load_fact=  PythonOperator(task_id = 'load_facts',
                                python_callable = load_facts,
                                op_kwargs={"context": "{{ execution_date }}"})

    


create_table >> fetch_dataa >> clean_data >> load_dim >> load_fact













