#COVID-19 Data Pipeline with Apache Airflow ğŸš€
#Automated Data Processing Pipeline for COVID-19 Case Data
#Extract | Transform | Load | Analyze

ğŸ“Œ Project Overview
The COVID-19 Data Pipeline is an Apache Airflow-powered ETL workflow that automates the process of fetching, cleaning, and loading global COVID-19 case data from Johns Hopkins University (JHU) into a Microsoft SQL Server database for further analysis.

This project demonstrates how to efficiently process over 4 million rows of pandemic data, handling missing values, data inconsistencies, and database optimization while ensuring the dataset is structured for analytical use.

âš¡ Key Features:
âœ… Automated Data Fetching: Downloads JHUâ€™s daily COVID-19 reports directly from GitHub.
âœ… Scalable ETL Pipeline: Processes large datasets efficiently using Airflowâ€™s DAGs.
âœ… Data Cleaning & Transformation: Standardizes country names, removes null values, and ensures consistency.
âœ… Database Optimization: Implements dimensional modeling for efficient querying.
âœ… Robust Error Handling: Skips missing files, logs errors, and maintains data integrity.

ğŸ“Š Project Architecture
ğŸ” Workflow of the Airflow DAG
pgsql
Copy
Edit
            +--------------------------+
            | Start DAG Execution       |
            +-----------+--------------+
                        |
                        v
            +--------------------------+
            | Create Database Schema    |
            | (Tables & Constraints)    |
            +-----------+--------------+
                        |
                        v
            +--------------------------+
            | Fetch Data from GitHub    |
            +-----------+--------------+
                        |
                        v
            +--------------------------+
            | Clean & Transform Data    |
            | (Standardize, Handle NULLs)|
            +-----------+--------------+
                        |
                        v
            +--------------------------+
            | Load into SQL Server      |
            +-----------+--------------+
                        |
                        v
            +--------------------------+
            | Update Facts Table        |
            | (Ensure Data Consistency) |
            +-----------+--------------+
                        |
                        v
            +--------------------------+
            | DAG Execution Complete    |
            +--------------------------+
ğŸ› ï¸ Tech Stack & Tools
Category	Technology Used
Orchestration	Apache Airflow
Data Extraction	Python (requests, pandas)
Database	Microsoft SQL Server
Database ORM	SQLAlchemy
Programming Language	Python
Task Scheduling	Airflow DAGs
ğŸ“‚ Project Structure
plaintext
Copy
Edit
Covid19-Airflow-Pipeline/
â”‚-- dags/
â”‚   â”œâ”€â”€ covid_analytics_dag.py  # Airflow DAG script
â”‚-- sql/
â”‚   â”œâ”€â”€ schema.sql  # SQL schema for the database
â”‚-- config/
â”‚   â”œâ”€â”€ airflow_settings.py  # Airflow configurations
â”‚-- README.md  # Project documentation
â”‚-- requirements.txt  # Required Python packages
â”‚-- .gitignore  # Git ignored files
ğŸ“ Detailed ETL Process
1ï¸âƒ£ Database Schema Creation (SCHEMA_SQL)
Creates three tables:

dim_date â†’ Stores unique dates

dim_location â†’ Stores unique locations with latitude & longitude

fact_covid_cases â†’ Stores daily case counts

2ï¸âƒ£ Data Extraction (fetch_data)
Pulls the latest COVID-19 reports from JHU's GitHub repository.

Handles missing files and skips execution when necessary.

Fetches & processes over 4 million rows of historical pandemic data.

3ï¸âƒ£ Data Transformation (clean_transform_data)
Standardizes column names for consistency.

Handles missing values & incorrect data types.

Maps country names for consistency in reports.

4ï¸âƒ£ Load Dimension Tables (load_dimensions)
Date dimension (dim_date) â†’ Inserts unique date values.

Location dimension (dim_location) â†’ Handles duplicates and missing coordinates.

5ï¸âƒ£ Load Fact Table (load_facts)
Ensures data integrity before inserting records into fact_covid_cases.

Updates existing records if new case counts are reported.

