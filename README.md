#COVID-19 Data Pipeline with Apache Airflow üöÄ
#Automated Data Processing Pipeline for COVID-19 Case Data
#Extract | Transform | Load | Analyze

üìå Project Overview
The COVID-19 Data Pipeline is an Apache Airflow-powered ETL workflow that automates the process of fetching, cleaning, and loading global COVID-19 case data from Johns Hopkins University (JHU) into a Microsoft SQL Server database for further analysis.

This project demonstrates how to efficiently process over 4 million rows of pandemic data, handling missing values, data inconsistencies, and database optimization while ensuring the dataset is structured for analytical use.

‚ö° Key Features:
‚úÖ Automated Data Fetching: Downloads JHU‚Äôs daily COVID-19 reports directly from GitHub.
‚úÖ Scalable ETL Pipeline: Processes large datasets efficiently using Airflow‚Äôs DAGs.
‚úÖ Data Cleaning & Transformation: Standardizes country names, removes null values, and ensures consistency.
‚úÖ Database Optimization: Implements dimensional modeling for efficient querying.
‚úÖ Robust Error Handling: Skips missing files, logs errors, and maintains data integrity.

üìä Project Architecture
üîÅ Workflow of the Airflow DAG
pgsql

            +--------------------------+
            | Start DAG Execution       |
            +-----------+--------------+
                        |
                        v
            +--------------------------+
            | Create Database Schema    |
            | (Tables & Constraints)    |
            +-----------+--------------+
                        |
                        v
            +--------------------------+
            | Fetch Data from GitHub    |
            +-----------+--------------+
                        |
                        v
            +--------------------------+
            | Clean & Transform Data    |
            | (Standardize, Handle NULLs)|
            +-----------+--------------+
                        |
                        v
            +--------------------------+
            | Load into SQL Server      |
            +-----------+--------------+
                        |
                        v
            +--------------------------+
            | Update Facts Table        |
            | (Ensure Data Consistency) |
            +-----------+--------------+
                        |
                        v
            +--------------------------+
            | DAG Execution Complete    |
            +--------------------------+


#üõ†Ô∏è Tech Stack & Tools
Category	Technology Used
Orchestration	Apache Airflow
Data Extraction	Python (requests, pandas)
Database	Microsoft SQL Server
Database ORM	SQLAlchemy
Programming Language	Python
Task Scheduling	Airflow DAGs



‚îÇ
üìù Detailed ETL Process
1Ô∏è‚É£ Database Schema Creation (SCHEMA_SQL)
Creates three tables:

dim_date ‚Üí Stores unique dates

dim_location ‚Üí Stores unique locations with latitude & longitude

fact_covid_cases ‚Üí Stores daily case counts

2Ô∏è‚É£ Data Extraction (fetch_data)
Pulls the latest COVID-19 reports from JHU's GitHub repository.

Handles missing files and skips execution when necessary.

Fetches & processes over 4 million rows of historical pandemic data.

3Ô∏è‚É£ Data Transformation (clean_transform_data)
Standardizes column names for consistency.

Handles missing values & incorrect data types.

Maps country names for consistency in reports.

4Ô∏è‚É£ Load Dimension Tables (load_dimensions)
Date dimension (dim_date) ‚Üí Inserts unique date values.

Location dimension (dim_location) ‚Üí Handles duplicates and missing coordinates.

5Ô∏è‚É£ Load Fact Table (load_facts)
Ensures data integrity before inserting records into fact_covid_cases.

Updates existing records if new case counts are reported.

